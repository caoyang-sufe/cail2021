{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#CAIL2021\" data-toc-modified-id=\"CAIL2021-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>CAIL2021</a></span></li><li><span><a href=\"#题库情况\" data-toc-modified-id=\"题库情况-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>题库情况</a></span></li><li><span><a href=\"#关于题库token2id与参考书目文档token2id对比\" data-toc-modified-id=\"关于题库token2id与参考书目文档token2id对比-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>关于题库token2id与参考书目文档token2id对比</a></span><ul class=\"toc-item\"><li><span><a href=\"#读取TOKEN2ID，TOKEN2FREQUENCY与REFERENCE_TOKEN2ID，REFERENCE_TOKEN2FREQUENCY并转为字典\" data-toc-modified-id=\"读取TOKEN2ID，TOKEN2FREQUENCY与REFERENCE_TOKEN2ID，REFERENCE_TOKEN2FREQUENCY并转为字典-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>读取TOKEN2ID，TOKEN2FREQUENCY与REFERENCE_TOKEN2ID，REFERENCE_TOKEN2FREQUENCY并转为字典</a></span></li><li><span><a href=\"#统计那些在参考书目中但不在题库的分词\" data-toc-modified-id=\"统计那些在参考书目中但不在题库的分词-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>统计那些在参考书目中但不在题库的分词</a></span></li><li><span><a href=\"#统计那些在题库中但不在参考书目的分词\" data-toc-modified-id=\"统计那些在题库中但不在参考书目的分词-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>统计那些在题库中但不在参考书目的分词</a></span></li><li><span><a href=\"#词频统计信息\" data-toc-modified-id=\"词频统计信息-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>词频统计信息</a></span></li></ul></li><li><span><a href=\"#gensim模型\" data-toc-modified-id=\"gensim模型-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>gensim模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#tfidf模型不同参数的情况\" data-toc-modified-id=\"tfidf模型不同参数的情况-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>tfidf模型不同参数的情况</a></span></li><li><span><a href=\"#Embedding模型测试\" data-toc-modified-id=\"Embedding模型测试-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Embedding模型测试</a></span></li></ul></li><li><span><a href=\"#BERT模型测试\" data-toc-modified-id=\"BERT模型测试-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>BERT模型测试</a></span></li><li><span><a href=\"#参考书目文档的高阶处理\" data-toc-modified-id=\"参考书目文档的高阶处理-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>参考书目文档的高阶处理</a></span><ul class=\"toc-item\"><li><span><a href=\"#统计字符种类及频率\" data-toc-modified-id=\"统计字符种类及频率-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>统计字符种类及频率</a></span></li><li><span><a href=\"#存储解析树\" data-toc-modified-id=\"存储解析树-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>存储解析树</a></span></li><li><span><a href=\"#句法树数据挖掘\" data-toc-modified-id=\"句法树数据挖掘-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>句法树数据挖掘</a></span><ul class=\"toc-item\"><li><span><a href=\"#关于句法树标注\" data-toc-modified-id=\"关于句法树标注-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;</span>关于句法树标注</a></span></li><li><span><a href=\"#句法树标注情况分析（以reference_parse_tree为例）\" data-toc-modified-id=\"句法树标注情况分析（以reference_parse_tree为例）-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;</span>句法树标注情况分析（以reference_parse_tree为例）</a></span></li><li><span><a href=\"#句法树标注情况分析（以question_parse_tree为例）\" data-toc-modified-id=\"句法树标注情况分析（以question_parse_tree为例）-6.3.3\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;</span>句法树标注情况分析（以question_parse_tree为例）</a></span></li><li><span><a href=\"#句法树的基本情况的一些特点挖掘\" data-toc-modified-id=\"句法树的基本情况的一些特点挖掘-6.3.4\"><span class=\"toc-item-num\">6.3.4&nbsp;&nbsp;</span>句法树的基本情况的一些特点挖掘</a></span></li><li><span><a href=\"#句法树生成networkx图测试\" data-toc-modified-id=\"句法树生成networkx图测试-6.3.5\"><span class=\"toc-item-num\">6.3.5&nbsp;&nbsp;</span>句法树生成networkx图测试</a></span></li><li><span><a href=\"#句法树分支数量情况（最大子女数量）\" data-toc-modified-id=\"句法树分支数量情况（最大子女数量）-6.3.6\"><span class=\"toc-item-num\">6.3.6&nbsp;&nbsp;</span>句法树分支数量情况（最大子女数量）</a></span></li><li><span><a href=\"#句法树在Dataset中的生成情况测试\" data-toc-modified-id=\"句法树在Dataset中的生成情况测试-6.3.7\"><span class=\"toc-item-num\">6.3.7&nbsp;&nbsp;</span>句法树在Dataset中的生成情况测试</a></span></li><li><span><a href=\"#use_reference为False的测试\" data-toc-modified-id=\"use_reference为False的测试-6.3.8\"><span class=\"toc-item-num\">6.3.8&nbsp;&nbsp;</span>use_reference为False的测试</a></span></li><li><span><a href=\"#use_reference为True的测试\" data-toc-modified-id=\"use_reference为True的测试-6.3.9\"><span class=\"toc-item-num\">6.3.9&nbsp;&nbsp;</span>use_reference为True的测试</a></span></li></ul></li><li><span><a href=\"#依存关系图\" data-toc-modified-id=\"依存关系图-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>依存关系图</a></span></li></ul></li><li><span><a href=\"#BERT输出本地化的一些测试\" data-toc-modified-id=\"BERT输出本地化的一些测试-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>BERT输出本地化的一些测试</a></span></li><li><span><a href=\"#图模型测试\" data-toc-modified-id=\"图模型测试-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>图模型测试</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAIL2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T08:31:10.251758Z",
     "start_time": "2022-09-07T08:31:04.415491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_dataloader' from 'src.dataset' (f:\\code\\project\\cail2021\\src\\dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5dc5a9bb3eea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_gensim_retrieval_models\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBasicDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerate_dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGensimRetrievalModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGensimEmbeddingModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'generate_dataloader' from 'src.dataset' (f:\\code\\project\\cail2021\\src\\dataset.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import dill\n",
    "import torch\n",
    "import numpy\n",
    "import gensim\n",
    "import pandas\n",
    "\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.models import Word2Vec, FastText, Doc2Vec, WordEmbeddingSimilarityIndex\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix\n",
    "\n",
    "\n",
    "\n",
    "from config import DatasetConfig, RetrievalModelConfig, EmbeddingModelConfig\n",
    "from setting import *\n",
    "from preprocess import build_gensim_retrieval_models\n",
    "\n",
    "from src.dataset import BasicDataset, generate_dataloader\n",
    "from src.retrieval_model import GensimRetrievalModel\n",
    "from src.embedding_model import GensimEmbeddingModel\n",
    "from src.evaluation_tools import evaluate_gensim_model_in_filling_subject\n",
    "from src.graph import Graph\n",
    "from src.graph_tools import generate_pos_tags, generate_parse_tree, generate_pos_tags_from_parse_tree, parse_tree_to_graph\n",
    "from src.qa_module import TreeNodeEncoder\n",
    "from src.utils import load_args, timer, is_chinese, is_number, is_alphabet, is_symbol\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 题库情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T06:48:08.549654Z",
     "start_time": "2022-01-04T06:48:08.314251Z"
    }
   },
   "outputs": [],
   "source": [
    "for path in RAW_TRAINSET_PATHs:\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    print(len(data))\n",
    "    \n",
    "for path in RAW_TESTSET_PATHs:\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    print(len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T06:49:45.144553Z",
     "start_time": "2022-01-04T06:49:45.113209Z"
    }
   },
   "outputs": [],
   "source": [
    "print(7775 + 13297)\n",
    "print(1986 + 3303)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关于题库token2id与参考书目文档token2id对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取TOKEN2ID，TOKEN2FREQUENCY与REFERENCE_TOKEN2ID，REFERENCE_TOKEN2FREQUENCY并转为字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T13:04:58.449396Z",
     "start_time": "2021-12-12T13:04:58.359293Z"
    }
   },
   "outputs": [],
   "source": [
    "token2id_df = pandas.read_csv(TOKEN2ID_PATH, sep='\\t', header=0)\n",
    "token2frequency_df = pandas.read_csv(TOKEN2FREQUENCY_PATH, sep='\\t', header=0)\n",
    "reference_token2id_df = pandas.read_csv(REFERENCE_TOKEN2ID_PATH, sep='\\t', header=0)\n",
    "reference_token2frequency_df = pandas.read_csv(REFERENCE_TOKEN2FREQUENCY_PATH, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T13:04:59.719587Z",
     "start_time": "2021-12-12T13:04:59.664406Z"
    }
   },
   "outputs": [],
   "source": [
    "token2id = {token: _id for token, _id in zip(token2id_df['token'], token2id_df['id'])}\n",
    "token2frequency = {token: frequency for token, frequency in zip(token2frequency_df['token'], token2frequency_df['frequency'])}\n",
    "reference_token2id = {token: _id for token, _id in zip(reference_token2id_df['token'], reference_token2id_df['id'])}\n",
    "reference_token2frequency = {token: frequency for token, frequency in zip(reference_token2frequency_df['token'], reference_token2frequency_df['frequency'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计那些在参考书目中但不在题库的分词\n",
    "\n",
    "结论：前三个都是标点符号，后面的词频都非常低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T13:05:01.604600Z",
     "start_time": "2021-12-12T13:05:01.583609Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "token2frequency_extra_in_reference = {}\n",
    "for token in reference_token2id:\n",
    "    if not token in token2id:\n",
    "        count += 1\n",
    "        token2frequency_extra_in_reference[token] = reference_token2frequency[token]\n",
    "\n",
    "print(len(token2frequency_extra_in_reference))\n",
    "print(Counter(token2frequency_extra_in_reference).most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计那些在题库中但不在参考书目的分词\n",
    "\n",
    "结论：最多的是顿号，然后大都是名词性短语（如人名，地名等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T13:05:02.857352Z",
     "start_time": "2021-12-12T13:05:02.814221Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "token2frequency_not_in_reference = {}\n",
    "for token in token2id:\n",
    "    if not token in reference_token2id:\n",
    "        count += 1\n",
    "        token2frequency_not_in_reference[token] = token2frequency[token]\n",
    "\n",
    "print(len(token2frequency_not_in_reference))\n",
    "print(Counter(token2frequency_not_in_reference).most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词频统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T13:07:32.859013Z",
     "start_time": "2021-12-12T13:07:31.622679Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = Counter(token2frequency)\n",
    "reference_Counter = Counter(reference_token2frequency)\n",
    "\n",
    "def _plot_frequency(_counter):\n",
    "    _y = numpy.array(list(map(lambda x: x[1], counter.most_common())), dtype=numpy.int64)\n",
    "    _x = numpy.array(list(range(_y.shape[0])))\n",
    "    \n",
    "    \n",
    "    for i in range(10):\n",
    "        plt.plot(_x[i * 100: (i + 1) * 100], _y[i * 100: (i + 1) * 100])\n",
    "        plt.show()\n",
    "\n",
    "_plot_frequency(counter)\n",
    "_plot_frequency(reference_Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T13:05:38.039705Z",
     "start_time": "2021-12-12T13:05:38.008889Z"
    }
   },
   "outputs": [],
   "source": [
    "token2frequency_df['frequency'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T13:05:45.598296Z",
     "start_time": "2021-12-12T13:05:45.582281Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_token2frequency_df['frequency'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf模型不同参数的情况\n",
    "\n",
    "目前看下来tfidf的效果最好，logentropy也不错，但是它没什么参数可调，word2vec和fasttext效果较好，但不如tfidf，并且我把词向量维数从256调为512后还是没有显著的性能提升，hdp效果一般，lsi较差，lda非常差，因此我只考虑在tfidf上做测试了，主要是smartirs, slope, pivot三个参数的调整。\n",
    "\n",
    "目前看下来hit@3的水平最高还是只有91.7%(6438个正确)，跟论文中比hit@1的水平基本持平TextCNN，但是hit@3要差BERT一点点，但是要经济得多\n",
    "\n",
    "tfidf调参结果是6438个，logentropy为6381个正确预测，总共7021个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T05:17:27.930339Z",
     "start_time": "2021-12-15T05:17:27.915204Z"
    }
   },
   "outputs": [],
   "source": [
    "summarys = json.load(open(os.path.join(TEMP_DIR, 'test_smartirs.json'), 'r')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T05:17:29.265941Z",
     "start_time": "2021-12-15T05:17:29.250603Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_dict = {\n",
    "    'smartirs': [],\n",
    "    'pivot': [],\n",
    "    'slope': [],\n",
    "    'hit@1': [],\n",
    "    'hit@3': [],\n",
    "    'hit@5': [],\n",
    "    'hit@10': [],\n",
    "}\n",
    "for summary in summarys:\n",
    "    \n",
    "    summary_dict['smartirs'].append(summary['args']['smartirs_tfidf'])\n",
    "    summary_dict['pivot'].append(summary['args']['pivot'])\n",
    "    summary_dict['slope'].append(summary['args']['slope'])\n",
    "    summary_dict['hit@1'].append(summary['result']['tfidf']['hit@1'])\n",
    "    summary_dict['hit@3'].append(summary['result']['tfidf']['hit@3'])\n",
    "    summary_dict['hit@5'].append(summary['result']['tfidf']['hit@5'])\n",
    "    summary_dict['hit@10'].append(summary['result']['tfidf']['hit@10'])\n",
    "    \n",
    "summary_dataframe = pandas.DataFrame(summary_dict, columns=list(summary_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T05:18:05.280803Z",
     "start_time": "2021-12-15T05:18:05.265388Z"
    }
   },
   "outputs": [],
   "source": [
    "print(summary_dataframe['hit@1'].max() / 7021)\n",
    "print(summary_dataframe['hit@3'].max() / 7021)\n",
    "print(summary_dataframe['hit@5'].max() / 7021)\n",
    "print(summary_dataframe['hit@10'].max() / 7021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T05:18:53.262638Z",
     "start_time": "2021-12-15T05:18:53.073971Z"
    }
   },
   "outputs": [],
   "source": [
    "print(summary_dataframe['hit@3'].max())\n",
    "print(summary_dataframe[summary_dataframe['hit@3'] == 6438])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T05:20:11.052544Z",
     "start_time": "2021-12-15T05:20:11.021103Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_dataframe.to_csv('summary.csv', header=True, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T13:15:27.230935Z",
     "start_time": "2022-01-04T13:15:22.599632Z"
    }
   },
   "outputs": [],
   "source": [
    "model_word2vec = gensim.models.Word2Vec.load(REFERENCE_WORD2VEC_MODEL_PATH)\n",
    "model_fasttext = gensim.models.Word2Vec.load(REFERENCE_FASTTEXT_MODEL_PATH)\n",
    "\n",
    "print(model_word2vec.wv.similarity('抢劫', '强奸'))\n",
    "print(model_fasttext.wv.similarity('抢劫', '强奸'))\n",
    "print(model_word2vec.wv.similarity('绑架', '盗窃'))\n",
    "print(model_fasttext.wv.similarity('绑架', '盗窃'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T13:13:34.761384Z",
     "start_time": "2022-01-04T13:13:34.749014Z"
    }
   },
   "outputs": [],
   "source": [
    "model_word2vec.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T06:33:08.007975Z",
     "start_time": "2022-01-04T06:33:07.821792Z"
    }
   },
   "outputs": [],
   "source": [
    "similarity_index = WordEmbeddingSimilarityIndex(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T06:34:55.922837Z",
     "start_time": "2022-01-04T06:33:09.661373Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load(REFERENCE_DICTIONARY_PATH)\n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T06:36:13.918033Z",
     "start_time": "2022-01-04T06:36:13.903065Z"
    }
   },
   "outputs": [],
   "source": [
    "help(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T06:36:19.614216Z",
     "start_time": "2022-01-04T06:36:19.597167Z"
    }
   },
   "outputs": [],
   "source": [
    "similarity_matrix[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T06:20:09.793927Z",
     "start_time": "2022-01-04T06:20:09.784413Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.similarity('抢劫', '司法')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T06:20:11.956043Z",
     "start_time": "2022-01-04T06:20:11.942934Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T06:20:13.465882Z",
     "start_time": "2022-01-04T06:20:13.462886Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.similar_by_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-04T06:21:22.115276Z",
     "start_time": "2022-01-04T06:21:22.101181Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv['强奸', '盗窃'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T13:01:45.614090Z",
     "start_time": "2021-12-27T13:01:45.598098Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.__getitem__('人民法院').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T13:01:47.151179Z",
     "start_time": "2021-12-27T13:01:47.135805Z"
    }
   },
   "outputs": [],
   "source": [
    "'人民法院aaa' in model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T13:01:48.740770Z",
     "start_time": "2021-12-27T13:01:48.724825Z"
    }
   },
   "outputs": [],
   "source": [
    "type(model.wv.__getitem__('人民法院'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T13:02:31.916247Z",
     "start_time": "2021-12-27T13:02:31.711714Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(REFERENCE_DOC2VEC_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-27T13:06:16.908486Z",
     "start_time": "2021-12-27T13:06:16.898887Z"
    }
   },
   "outputs": [],
   "source": [
    "model.infer_vector(['在', '法定代表', '人', '和', '法人', '关系', '的', '问题', '上', '，', '下列', '哪些', '表述', '是', '正确', '的']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @author: caoyang\n",
    "# @email: caoyang@163.sufe.edu.cn\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas\n",
    "\n",
    "from config import DatasetConfig, RetrievalModelConfig, EmbeddingModelConfig\n",
    "from setting import *\n",
    "\n",
    "from src.dataset import Dataset\n",
    "from src.retrieval_model import GensimRetrievalModel\n",
    "from src.embedding_model import GensimEmbeddingModel, TransformersEmbeddingModel\n",
    "from src.evaluation_tools import evaluate_gensim_model_in_filling_subject\n",
    "from src.plot_tools import train_plot_choice, train_plot_judgment\n",
    "from src.utils import load_args\n",
    "\n",
    "args = load_args(Config=EmbeddingModelConfig)\n",
    "\n",
    "# tem = TransformersEmbeddingModel(args=args)\n",
    "# tokenizer, model = tem.load_bert_model()\n",
    "\n",
    "tokenizer, model = TransformersEmbeddingModel.load_bert_model()\n",
    "\n",
    "\n",
    "\n",
    "text = [\n",
    "\t'劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。劳动法有广义和狭义之分:狭义上的劳动法，一般是指国家最高立法机构制定颁布的全国性.练合性的劳动法，即《中华人民共和国劳动法》(以下简称《劳动法》)；广义上的劳动法，是指调整劳动关系以及与劳动关系有密切联系的其他社会关系的法律规范的总称。',\n",
    "] * 1\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "print(encoded_input)\n",
    "\n",
    "t = time.time()\n",
    "for i in range(1):\n",
    "\toutput = model(**encoded_input)\n",
    "print(time.time() - t)\n",
    "\n",
    "input('pause')\n",
    "\n",
    "\n",
    "print('#' * 64)\n",
    "print('last_hidden_state: ', output.get('last_hidden_state').shape)\n",
    "print('pooler_output', output.get('pooler_output').shape)\n",
    "print('cross_attentions', output.cross_attentions)\n",
    "print('hidden_states', output.hidden_states)\n",
    "print('#' * 64)\n",
    "\n",
    "input()\n",
    "\n",
    "for encoded_hidden_state in output.last_hidden_state:\n",
    "\ti = 0\n",
    "\tfor layer in encoded_hidden_state:\n",
    "\t\ti += 1\n",
    "\t\tprint(i, layer)\n",
    "\t\tinput()\n",
    "\t\t\n",
    "\n",
    "print('#' * 64)\n",
    "for encoded_text in output.pooler_output:\n",
    "\tprint(encoded_text)\n",
    "\tprint('-*' * 32)\n",
    "\n",
    "'''\n",
    "last_hidden_state:  torch.Size([1, 120, 768])\n",
    "pooler_output torch.Size([1, 768])\n",
    "'''\n",
    "\n",
    "# print(output.logits)\n",
    "\n",
    "['__annotations__', '__class__', '__contains__', '__dataclass_fields__', \n",
    "'__dataclass_params__', '__delattr__', '__delitem__', '__dict__', \n",
    "'__dir__', '__doc__', '__eq__', '__format__', '__ge__', \n",
    "'__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', \n",
    "'__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', \n",
    "'__module__', '__ne__', '__new__', '__post_init__', '__reduce__', \n",
    "'__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__setitem__', \n",
    "'__sizeof__', '__str__', '__subclasshook__', \n",
    "'attentions', 'clear', 'copy', 'cross_attentions', 'fromkeys', 'get', \n",
    "'hidden_states', 'items', 'keys', 'last_hidden_state', 'move_to_end', \n",
    "'past_key_values', 'pooler_output', 'pop', 'popitem', 'setdefault', \n",
    "'to_tuple', 'update', 'values']\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "# classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "# sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "# sequence_1 = \"Apples are especially bad for your health\"\n",
    "# sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "# paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "# not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "# paraphrase_classification_logits = model(**paraphrase).logits\n",
    "# not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "# paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "# not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "# # Should be paraphrase\n",
    "# for i in range(len(classes)):\n",
    "\t# print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "# # Should not be paraphrase\n",
    "# for i in range(len(classes)):\n",
    "\t# print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考书目文档的高阶处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计字符种类及频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T15:41:47.526923Z",
     "start_time": "2022-05-07T15:41:44.842235Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_dataframe = pandas.read_csv(REFERENCE_PATH, sep='\\t', header=0)\n",
    "\n",
    "numbers = []\n",
    "alphabets = []\n",
    "symbols = []\n",
    "\n",
    "for i in range(reference_dataframe.shape[0]):\n",
    "    content_tokens = eval(reference_dataframe.loc[i, 'content'])\n",
    "    for token in content_tokens:\n",
    "        if is_chinese(token):\n",
    "            continue\n",
    "        elif is_number(token):\n",
    "            # print(f'number: {token}')\n",
    "            numbers.append(token)\n",
    "        elif is_alphabet(token):\n",
    "            # print(f'alphabet: {token}')\n",
    "            alphabets.append(token)\n",
    "        else:\n",
    "            # print(f'symbol: {token}')\n",
    "            symbols.append(token)\n",
    "            \n",
    "# print(set(numbers))\n",
    "# print(set(alphabets))\n",
    "print(set(symbols))\n",
    "# print(Counter(numbers).most_common())\n",
    "# print(Counter(alphabets).most_common())\n",
    "print(Counter(symbols).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T15:41:58.092131Z",
     "start_time": "2022-05-07T15:41:58.076836Z"
    }
   },
   "outputs": [],
   "source": [
    "print(reference_dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存储解析树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T12:18:53.252572Z",
     "start_time": "2022-03-14T12:18:52.750807Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_dataframe = pandas.read_csv(REFERENCE_PARSE_TREE_PATH, sep='\\t', header=0, dtype=str)\n",
    "reference_dataframe = pandas.read_csv(REFERENCE_PATH, sep='\\t', header=0, dtype=str)\n",
    "reference_dataframe = reference_dataframe.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T12:18:53.560918Z",
     "start_time": "2022-03-14T12:18:53.550758Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tree_dataframe.shape)\n",
    "print(reference_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T12:18:54.281033Z",
     "start_time": "2022-03-14T12:18:54.250607Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T12:18:54.980652Z",
     "start_time": "2022-03-14T12:18:54.950777Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_dataframe['error'] = tree_dataframe['parse_tree'].map(lambda x: not x[:7] == \"['(ROOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T12:18:55.546627Z",
     "start_time": "2022-03-14T12:18:55.530750Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_dataframe[tree_dataframe['error']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T12:24:34.195008Z",
     "start_time": "2022-03-14T12:24:34.180701Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_dataframe[tree_dataframe['error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T12:25:36.979635Z",
     "start_time": "2022-03-14T12:25:36.965579Z"
    }
   },
   "outputs": [],
   "source": [
    "content_tokens = eval(reference_dataframe.loc[19592, 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-14T12:25:40.566801Z",
     "start_time": "2022-03-14T12:25:40.547803Z"
    }
   },
   "outputs": [],
   "source": [
    "content_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句法树数据挖掘\n",
    "\n",
    "- reference句法树的词性标注（即叶子节点上的标注）共计33种，具体解释见[https://blog.csdn.net/weixin_30642561/article/details/97772970?spm=1001.2101.3001.6650.10&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-10.pc_relevant_antiscanv2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-10.pc_relevant_antiscanv2&utm_relevant_index=14](https://blog.csdn.net/weixin_30642561/article/details/97772970?spm=1001.2101.3001.6650.10&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-10.pc_relevant_antiscanv2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-10.pc_relevant_antiscanv2&utm_relevant_index=14)\n",
    "\n",
    "- reference句法树的词性标注（即叶子节点上的标注）共计28种，具体解释见[https://blog.csdn.net/zkq_1986/article/details/93175100](https://blog.csdn.net/zkq_1986/article/details/93175100)（不确定）\n",
    "\n",
    "- jieba的标注可见[https://blog.csdn.net/kevin_darkelf/article/details/39520881](https://blog.csdn.net/kevin_darkelf/article/details/39520881)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于句法树标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T04:14:41.352613Z",
     "start_time": "2022-03-30T04:14:41.321149Z"
    }
   },
   "outputs": [],
   "source": [
    "s = \"\"\"ROOT：要处理文本的语句\n",
    "\n",
    "IP：简单从句\n",
    "\n",
    "NP：名词短语\n",
    "\n",
    "VP：动词短语\n",
    "\n",
    "PU：断句符，通常是句号、问号、感叹号等标点符号\n",
    "\n",
    "LCP：方位词短语\n",
    "\n",
    "PP：介词短语\n",
    "\n",
    "CP：由‘的’构成的表示修饰性关系的短语\n",
    "\n",
    "DNP：由‘的’构成的表示所属关系的短语\n",
    "\n",
    "ADVP：副词短语\n",
    "\n",
    "ADJP：形容词短语\n",
    "\n",
    "DP：限定词短语\n",
    "\n",
    "QP：量词短语\n",
    "\n",
    "NN：常用名词\n",
    "\n",
    "NR：固有名词\n",
    "\n",
    "NT：时间名词\n",
    "\n",
    "PN：代词\n",
    "\n",
    "VV：动词\n",
    "\n",
    "VC：是\n",
    "\n",
    "CC：表示连词\n",
    "\n",
    "VE：有\n",
    "\n",
    "VA：表语形容词\n",
    "\n",
    "AS：内容标记（如：了）\n",
    "\n",
    "VRD：动补复合词\n",
    "\n",
    "CD: 表示基数词\n",
    "\n",
    "DT: determiner 表示限定词\n",
    "\n",
    "EX: existential there 存在句\n",
    "\n",
    "FW: foreign word 外来词\n",
    "\n",
    "IN: preposition or conjunction, subordinating 介词或从属连词\n",
    "\n",
    "JJ: adjective or numeral, ordinal 形容词或序数词\n",
    "\n",
    "JJR: adjective, comparative 形容词比较级\n",
    "\n",
    "JJS: adjective, superlative 形容词最高级\n",
    "\n",
    "LS: list item marker 列表标识\n",
    "\n",
    "MD: modal auxiliary 情态助动词\n",
    "\n",
    "PDT: pre-determiner 前位限定词\n",
    "\n",
    "POS: genitive marker 所有格标记\n",
    "\n",
    "PRP: pronoun, personal 人称代词\n",
    "\n",
    "RB: adverb 副词\n",
    "\n",
    "RBR: adverb, comparative 副词比较级\n",
    "\n",
    "RBS: adverb, superlative 副词最高级\n",
    "\n",
    "RP: particle 小品词\n",
    "\n",
    "SYM: symbol 符号\n",
    "\n",
    "TO:”to” as preposition or infinitive marker 作为介词或不定式标记\n",
    "\n",
    "WDT: WH-determiner WH限定词\n",
    "\n",
    "WP: WH-pronoun WH代词\n",
    "\n",
    "WP$: WH-pronoun, possessive WH所有格代词\n",
    "\n",
    "WRB:Wh-adverb WH副词\"\"\"\n",
    "\n",
    "lines = list(filter(None, s.splitlines()))\n",
    "labels = [line.replace('：', ':').split(':')[0].replace(' ', '') for line in lines]\n",
    "print(len(labels))\n",
    "print(len(set(labels)))\n",
    "labels = set(labels)\n",
    "\n",
    "mylabels = set(['ADJP', 'ADVP', 'CLP', 'CP', 'DFL', 'DNP', 'DP', 'DVP', 'FLR', 'FRAG', 'INC', 'INTJ', 'IP', 'LCP', 'LST', 'NP', 'PP', 'PRN', 'QP', 'ROOT', 'UCP', 'VCD', 'VCP', 'VNV', 'VP', 'VPT', 'VRD', 'VSB'])\n",
    "\n",
    "print(mylabels - labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句法树标注情况分析（以reference_parse_tree为例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T10:36:23.351069Z",
     "start_time": "2022-03-30T10:36:22.990778Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(REFERENCE_PARSE_TREE_PATH, sep='\\t', header=0)\n",
    "df['id'] = df['id'].astype(int)\n",
    "\n",
    "token2id = pandas.read_csv(REFERENCE_TOKEN2ID_PATH, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T12:33:47.298814Z",
     "start_time": "2022-03-29T12:33:47.282816Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T12:39:45.977140Z",
     "start_time": "2022-03-29T12:39:45.954134Z"
    }
   },
   "outputs": [],
   "source": [
    "token2id[token2id['token'] == '\\xa0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T04:14:00.673507Z",
     "start_time": "2022-03-30T04:14:00.642261Z"
    }
   },
   "outputs": [],
   "source": [
    "# 寻找解析错误的语句: 解析正确的语句开头一定是['(ROOT\n",
    "for i, content in enumerate(df['content']):\n",
    "    if not content.startswith(\"['(ROOT\"):\n",
    "        print(i, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:18:11.217662Z",
     "start_time": "2022-03-29T13:18:09.986626Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计所有的句法树标注\n",
    "labels1 = []\n",
    "\n",
    "for i, content in enumerate(df['content']):\n",
    "    parse_trees = eval(content)\n",
    "    for parse_tree in parse_trees:\n",
    "        string = parse_tree.replace('(', ' ').replace(')', ' ').strip()\n",
    "        tokens = string.split()\n",
    "        labels1.extend(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T04:16:53.460862Z",
     "start_time": "2022-03-30T04:16:53.178171Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计所有的句法树标注（排除所有叶子节点内容，我感觉叶子节点标注的是词性，而非句法树标注，使用正则替换消除）\n",
    "\n",
    "labels2 = []\n",
    "regex_compiler = re.compile(r'\\([^\\(\\)]+\\)', re.I)\n",
    "for i, content in enumerate(df['content']):\n",
    "    parse_trees = eval(content)\n",
    "    for parse_tree in parse_trees:\n",
    "        string = regex_compiler.sub('', parse_tree)\n",
    "        string = string.replace('(', ' ').replace(')', ' ').strip()\n",
    "        tokens = string.split()\n",
    "        labels2.extend(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T04:14:24.970231Z",
     "start_time": "2022-03-30T04:14:24.812761Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = Counter(labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T04:14:32.360540Z",
     "start_time": "2022-03-30T04:14:32.328870Z"
    }
   },
   "outputs": [],
   "source": [
    "counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T04:15:20.507173Z",
     "start_time": "2022-03-30T04:15:20.491553Z"
    }
   },
   "outputs": [],
   "source": [
    "error = ['INTJ', 'INC', 'DFL', 'LST', 'VCP', 'PRN', 'VPT', 'VNV', 'UCP', 'VCD', 'VSB', 'FRAG', 'DVP', 'FLR', 'CLP']\n",
    "for x in error:\n",
    "    print(x, counter.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:18:17.441991Z",
     "start_time": "2022-03-29T13:18:15.415366Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计所有的句法树标注（只抓叶子节点中的内容，我想看看跟上面一个标注是否有重复）\n",
    "labels3 = []\n",
    "regex_compiler = re.compile(r'\\([^\\(\\)]+\\)', re.I)\n",
    "for i, content in enumerate(df['content']):\n",
    "    parse_trees = eval(content)\n",
    "    for parse_tree in parse_trees:\n",
    "        results = regex_compiler.findall(parse_tree)\n",
    "        for result in results:\n",
    "            assert result[0] == '('\n",
    "            labels3.append(result.split()[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:18:37.997707Z",
     "start_time": "2022-03-29T13:18:37.387638Z"
    }
   },
   "outputs": [],
   "source": [
    "labels1 = set(labels1)\n",
    "labels2 = set(labels2)\n",
    "labels3 = set(labels3)\n",
    "reference_tokens = token2id['token'].unique()\n",
    "reference_tokens = set(reference_tokens)\n",
    "print(len(labels1))\n",
    "print(len(labels2))\n",
    "print(len(labels3))\n",
    "print(len(reference_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:19:26.592869Z",
     "start_time": "2022-03-29T13:19:26.571865Z"
    }
   },
   "outputs": [],
   "source": [
    "true_labels1 = labels1 - reference_tokens\n",
    "true_labels2 = labels2 - reference_tokens\n",
    "true_labels3 = labels3 - reference_tokens\n",
    "print(len(true_labels1))\n",
    "print(len(true_labels2))\n",
    "print(len(true_labels3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:20:14.162198Z",
     "start_time": "2022-03-29T13:20:14.149688Z"
    }
   },
   "outputs": [],
   "source": [
    "print(labels2.intersection(reference_tokens))\n",
    "print(labels3.intersection(reference_tokens))\n",
    "print(labels2.intersection(labels3)) # 是空集，这样的结果非常赞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:26:40.168916Z",
     "start_time": "2022-03-29T13:26:40.154911Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sorted(labels2))\n",
    "print(sorted(labels3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:30:45.338033Z",
     "start_time": "2022-03-29T13:30:45.325014Z"
    }
   },
   "outputs": [],
   "source": [
    "all_labels = labels2.union(labels3)\n",
    "print(len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:31:06.952811Z",
     "start_time": "2022-03-29T13:31:06.946802Z"
    }
   },
   "outputs": [],
   "source": [
    "LABELS2.issubset(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:31:18.193862Z",
     "start_time": "2022-03-29T13:31:18.182858Z"
    }
   },
   "outputs": [],
   "source": [
    "all_labels - LABELS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:22:15.247160Z",
     "start_time": "2022-03-29T13:22:15.233156Z"
    }
   },
   "outputs": [],
   "source": [
    "LABELS2 = {'ROOT', 'IP', 'NP', 'VP', 'PU', 'LCP', 'PP', 'CP', 'DNP', 'ADVP', 'ADJP', 'DP', 'QP', 'NN', 'NR', 'NT', 'PN', 'VV', 'VC', 'VA', 'CC', 'VE', 'AS', 'VRD'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:22:32.193410Z",
     "start_time": "2022-03-29T13:22:32.180406Z"
    }
   },
   "outputs": [],
   "source": [
    "LABELS2 - labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:22:49.349715Z",
     "start_time": "2022-03-29T13:22:49.332685Z"
    }
   },
   "outputs": [],
   "source": [
    "LABELS2 - labels3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:23:07.524780Z",
     "start_time": "2022-03-29T13:23:07.506933Z"
    }
   },
   "outputs": [],
   "source": [
    "labels2 - LABELS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:13:20.873389Z",
     "start_time": "2022-03-29T13:13:20.853385Z"
    }
   },
   "outputs": [],
   "source": [
    "# 出现在reference_tokens中但没有出现在句法解析树中的token（这是不正常的，因为理论上reference_tokens都出现在解析树中了）\n",
    "reference_tokens - labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:14:36.733691Z",
     "start_time": "2022-03-29T13:14:36.720452Z"
    }
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:10:41.549855Z",
     "start_time": "2022-03-29T13:10:41.543849Z"
    }
   },
   "outputs": [],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句法树标注情况分析（以question_parse_tree为例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-17T14:32:36.625157Z",
     "start_time": "2022-05-17T14:32:36.104915Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(QUESTION_PARSE_TREE_PATH, sep='\\t', header=0, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-17T14:32:58.840093Z",
     "start_time": "2022-05-17T14:32:58.824830Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['id'] == '1_3906']['statement'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION_PARSE_TREE中的确没有解析错误的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-14T13:34:58.941612Z",
     "start_time": "2022-05-14T13:34:58.091951Z"
    }
   },
   "outputs": [],
   "source": [
    "# 寻找解析错误的语句: 解析正确的语句开头一定是['(ROOT\n",
    "for i in range(df.shape[0]):\n",
    "    _id = df.loc[i, 'id']\n",
    "    statement = df.loc[i, 'statement']\n",
    "    option_a = df.loc[i, 'option_a']\n",
    "    option_b = df.loc[i, 'option_b']\n",
    "    option_c = df.loc[i, 'option_c']\n",
    "    option_d = df.loc[i, 'option_d']\n",
    "    if not (statement.startswith(\"['(ROOT\") or statement.startswith('[\"(ROOT')):\n",
    "        print(_id, 'statement', statement)\n",
    "    if not (option_a.startswith(\"['(ROOT\") or option_a.startswith('[\"(ROOT')):\n",
    "        print(_id, 'option_a', option_a)\n",
    "    if not (option_b.startswith(\"['(ROOT\") or option_b.startswith('[\"(ROOT')):\n",
    "        print(_id, 'option_b', option_b)\n",
    "    if not (option_c.startswith(\"['(ROOT\") or option_c.startswith('[\"(ROOT')):\n",
    "        print(_id, 'option_c', option_c)\n",
    "    if not (option_d.startswith(\"['(ROOT\") or option_d.startswith('[\"(ROOT')):\n",
    "        print(_id, 'option_d', option_d)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T03:56:22.030863Z",
     "start_time": "2022-04-10T03:56:19.489416Z"
    }
   },
   "outputs": [],
   "source": [
    "# 是否可以不使用eval，直接进行generate_pos_tags_from_parse_tree?\n",
    "\n",
    "def _f(_x):\n",
    "    _x = eval(_x)\n",
    "    _res = []\n",
    "    for _i in _x:\n",
    "        _res.extend(generate_pos_tags_from_parse_tree(_i))\n",
    "    \n",
    "    return _res\n",
    "\n",
    "array1 = df['statement'].map(generate_pos_tags_from_parse_tree)\n",
    "array2 = df['statement'].map(_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T03:56:47.594457Z",
     "start_time": "2022-04-10T03:56:47.516037Z"
    }
   },
   "outputs": [],
   "source": [
    "# 答：不可以，因为eval之后可以预先处理掉一些转义符\n",
    "# 下面两个数字不相等的原因就是不eval会把转义符识别为字符\n",
    "\n",
    "print(len(array1))\n",
    "print(sum(array1==array2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T03:57:38.018156Z",
     "start_time": "2022-04-10T03:57:37.939343Z"
    }
   },
   "outputs": [],
   "source": [
    "for a, b in zip(array1, array2):\n",
    "    if not a == b:\n",
    "        print(a)\n",
    "        print(b)\n",
    "        print('#' * 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T07:34:37.331430Z",
     "start_time": "2022-04-10T07:34:31.971521Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计所有的句法树标注（排除所有叶子节点内容，我感觉叶子节点标注的是词性，而非句法树标注，使用正则替换消除）\n",
    "labels2 = []\n",
    "regex_compiler = re.compile(r'\\([^\\(\\)]+\\)', re.I)\n",
    "\n",
    "for column in ['statement', 'option_a', 'option_b', 'option_c', 'option_d']:\n",
    "    for i, content in enumerate(df[column]):\n",
    "        parse_trees = eval(content)\n",
    "        for parse_tree in parse_trees:\n",
    "            string = regex_compiler.sub('', parse_tree)\n",
    "            string = string.replace('(', ' ').replace(')', ' ').strip()\n",
    "            tokens = string.split()\n",
    "            labels2.extend(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T07:34:53.654165Z",
     "start_time": "2022-04-10T07:34:47.164942Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计所有的句法树标注（只抓叶子节点中的内容，我想看看跟上面一个标注是否有重复）\n",
    "labels3 = []\n",
    "regex_compiler = re.compile(r'\\([^\\(\\)]+\\)', re.I)\n",
    "for column in ['statement', 'option_a', 'option_b', 'option_c', 'option_d']:\n",
    "    for i, content in enumerate(df[column]):\n",
    "        parse_trees = eval(content)\n",
    "        for parse_tree in parse_trees:\n",
    "            results = regex_compiler.findall(parse_tree)\n",
    "            for result in results:\n",
    "                assert result[0] == '('\n",
    "                labels3.append(result.split()[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T07:36:50.196602Z",
     "start_time": "2022-04-10T07:36:50.181621Z"
    }
   },
   "outputs": [],
   "source": [
    "labels2 = sorted(list(set(labels2)))\n",
    "labels3 = sorted(list(set(labels3)))\n",
    "\n",
    "print(f'句法树标注({len(labels2)}): {labels2}')\n",
    "print(f'词性标注({len(labels3)}): {labels3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明QUESTION_PARSE_TREE里的标注与REFERENCE_PARSE_TREE是完全相同的，只是句法树标注少一个INTJ（）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T07:39:18.056293Z",
     "start_time": "2022-04-10T07:39:18.034762Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in labels2:\n",
    "    assert i in STANFORD_SYNTACTIC_TAG, i\n",
    "\n",
    "for j in labels3:\n",
    "    assert j in STANFORD_POS_TAG, j\n",
    "    \n",
    "for i in STANFORD_SYNTACTIC_TAG:\n",
    "    assert i in labels2, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T13:09:42.295689Z",
     "start_time": "2022-03-29T13:09:42.275959Z"
    }
   },
   "source": [
    "### 句法树的基本情况的一些特点挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T11:21:15.929052Z",
     "start_time": "2022-09-07T11:21:13.440256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24718, 5)\n",
      "Index(['law', 'chapter_number', 'chapter_name', 'section', 'content'], dtype='object')\n",
      "(24718, 2)\n",
      "Index(['id', 'content'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 是否保留原序列的顺序？（是）\n",
    "\n",
    "reference_df = pandas.read_csv(REFERENCE_PATH, sep='\\t')\n",
    "reference_parse_tree_df = pandas.read_csv(REFERENCE_PARSE_TREE_PATH, sep='\\t', header=0)\n",
    "\n",
    "reference_df['content'] = reference_df['content'].map(eval)\n",
    "\n",
    "print(reference_df.shape)\n",
    "print(reference_df.columns)\n",
    "print(reference_parse_tree_df.shape)\n",
    "print(reference_parse_tree_df.columns)\n",
    "\n",
    "assert reference_df.shape[0] == reference_parse_tree_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 下面的脚本验证了句法树中的分词序列位置相较于原序列是没有发生改变的，不过有几个特殊符号的替换：\n",
    "\n",
    "   - 原分词序列中的'('与')'队以ing句法树中的'-LRB-'与'-RRB-'\n",
    "   - 原分词序列中的'\\xa0'（这个其实就是空格，只有jieba分词会出现这个东西），在句法树中的分词对应为空\n",
    "\n",
    "2. 下面的脚本说明只要做如上的几个替换，句法树里的分词序列与原序列是完全相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T11:21:41.686411Z",
     "start_time": "2022-09-07T11:21:39.633984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参考文档中不能匹配的数量: 0\n"
     ]
    }
   ],
   "source": [
    "error = []\n",
    "\n",
    "for i in range(reference_df.shape[0]):\n",
    "    content = reference_df.loc[i, 'content']\n",
    "    content_from_parse_tree = generate_pos_tags_from_parse_tree(parse_tree=reference_parse_tree_df.loc[i, 'content'])\n",
    "\n",
    "    content_ex = list(map(lambda x: x[1], content_from_parse_tree))\n",
    "    content_ex = list(map(lambda x: '(' if x == '-LRB-' else ')' if x == '-RRB-' else '\\xa0' if x == '' else x, content_ex))\n",
    "    # 若使用下面的方法，则会跳出几个error\n",
    "    # content_ex = list(map(lambda x: '(' if x == '-LRB-' else ')' if x == '-RRB-' else x, content_ex))           \n",
    "\n",
    "    if content == content_ex:\n",
    "        continue\n",
    "    else:\n",
    "        print(i)\n",
    "        error.append(i)\n",
    "\n",
    "print(f'参考文档中不能匹配的数量: {len(error)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面：类似地在question数据集上也进行与reference相同地测试，结论是相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T11:24:31.420356Z",
     "start_time": "2022-09-07T11:24:27.168384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26361, 9)\n",
      "(26361, 7)\n",
      "Index(['id', 'statement', 'option_a', 'option_b', 'option_c', 'option_d',\n",
      "       'type', 'subject', 'answer'],\n",
      "      dtype='object')\n",
      "Index(['id', 'statement', 'option_a', 'option_b', 'option_c', 'option_d',\n",
      "       'source'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 是否保留原序列的顺序？（是）\n",
    "\n",
    "filepaths = TRAINSET_PATHs + VALIDSET_PATHs + TESTSET_PATHs\n",
    "\n",
    "question_df = pandas.concat([pandas.read_csv(filepath, sep='\\t', header=0) for filepath in filepaths]).reset_index(drop=True)\n",
    "print(question_df.shape)\n",
    "\n",
    "question_parse_tree_df = pandas.read_csv(QUESTION_PARSE_TREE_PATH, sep='\\t', header=0)\n",
    "print(question_parse_tree_df.shape)\n",
    "\n",
    "question_df['statement'] = question_df['statement'].map(eval)\n",
    "question_df['option_a'] = question_df['option_a'].map(eval)\n",
    "question_df['option_b'] = question_df['option_b'].map(eval)\n",
    "question_df['option_c'] = question_df['option_c'].map(eval)\n",
    "question_df['option_d'] = question_df['option_d'].map(eval)\n",
    "\n",
    "print(question_df.columns)\n",
    "print(question_parse_tree_df.columns)\n",
    "assert reference_df.shape[0] == reference_parse_tree_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T11:39:17.975901Z",
     "start_time": "2022-09-07T11:39:14.481864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "题库中不能匹配的数量\n",
      "statement: 0\n",
      "option_a: 0\n",
      "option_b: 0\n",
      "option_c: 0\n",
      "option_d: 0\n"
     ]
    }
   ],
   "source": [
    "error_statement = []\n",
    "error_option_a = []\n",
    "error_option_b = []\n",
    "error_option_c = []\n",
    "error_option_d = []\n",
    "\n",
    "question_parse_tree_dict = {question_parse_tree_df.loc[i, 'id']: (question_parse_tree_df.loc[i, 'statement'], \n",
    "                                                                  question_parse_tree_df.loc[i, 'option_a'],\n",
    "                                                                  question_parse_tree_df.loc[i, 'option_b'], \n",
    "                                                                  question_parse_tree_df.loc[i, 'option_c'],\n",
    "                                                                  question_parse_tree_df.loc[i, 'option_d']) for i in range(question_parse_tree_df.shape[0])}\n",
    "\n",
    "for i in range(question_df.shape[0]):\n",
    "    _id = question_df.loc[i, 'id']\n",
    "    statement = question_df.loc[i, 'statement']\n",
    "    option_a = question_df.loc[i, 'option_a']\n",
    "    option_b = question_df.loc[i, 'option_b']\n",
    "    option_c = question_df.loc[i, 'option_c']\n",
    "    option_d = question_df.loc[i, 'option_d']\n",
    "    \n",
    "    statement_tree, option_a_tree, option_b_tree, option_c_tree, option_d_tree = question_parse_tree_dict[_id]   \n",
    "    \n",
    "    statement_from_parse_tree = generate_pos_tags_from_parse_tree(parse_tree=statement_tree)\n",
    "    option_a_from_parse_tree = generate_pos_tags_from_parse_tree(parse_tree=option_a)\n",
    "    option_b_from_parse_tree = generate_pos_tags_from_parse_tree(parse_tree=option_b)\n",
    "    option_c_from_parse_tree = generate_pos_tags_from_parse_tree(parse_tree=option_c)\n",
    "    option_d_from_parse_tree = generate_pos_tags_from_parse_tree(parse_tree=option_d)\n",
    "\n",
    "    statement_ex = list(map(lambda x: x[1], statement_from_parse_tree))\n",
    "    statement_ex = list(map(lambda x: '(' if x == '-LRB-' else ')' if x == '-RRB-' else '\\xa0' if x == '' else x, statement_ex))\n",
    "    if not content == content_ex:\n",
    "        print('statement', i)\n",
    "        error_statement.append(i)\n",
    "        \n",
    "    option_a_ex = list(map(lambda x: x[1], option_a_from_parse_tree))\n",
    "    option_a_ex = list(map(lambda x: '(' if x == '-LRB-' else ')' if x == '-RRB-' else '\\xa0' if x == '' else x, option_a_ex))\n",
    "    if not content == content_ex:\n",
    "        print('option_a', i)\n",
    "        error_option_a.append(i)\n",
    "\n",
    "    option_b_ex = list(map(lambda x: x[1], option_b_from_parse_tree))\n",
    "    option_b_ex = list(map(lambda x: '(' if x == '-LRB-' else ')' if x == '-RRB-' else '\\xa0' if x == '' else x, option_b_ex))\n",
    "    if not content == content_ex:\n",
    "        print('option_b', i)\n",
    "        error_option_b.append(i)\n",
    "\n",
    "    option_c_ex = list(map(lambda x: x[1], option_c_from_parse_tree))\n",
    "    option_c_ex = list(map(lambda x: '(' if x == '-LRB-' else ')' if x == '-RRB-' else '\\xa0' if x == '' else x, option_c_ex))\n",
    "    if not content == content_ex:\n",
    "        print('option_c', i)\n",
    "        error_option_c.append(i)\n",
    "\n",
    "    option_d_ex = list(map(lambda x: x[1], option_d_from_parse_tree))\n",
    "    option_d_ex = list(map(lambda x: '(' if x == '-LRB-' else ')' if x == '-RRB-' else '\\xa0' if x == '' else x, option_d_ex))\n",
    "    if not content == content_ex:\n",
    "        print('option_d', i)\n",
    "        error_option_d.append(i)\n",
    "\n",
    "\n",
    "print('题库中不能匹配的数量')\n",
    "print(f'statement: {len(error_statement)}')\n",
    "print(f'option_a: {len(error_option_a)}')\n",
    "print(f'option_b: {len(error_option_b)}')\n",
    "print(f'option_c: {len(error_option_c)}')\n",
    "print(f'option_d: {len(error_option_d)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句法树生成networkx图测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T15:55:44.240586Z",
     "start_time": "2022-04-28T15:55:44.226562Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_trees_to_graph(parse_trees, succ_to, error_to):\n",
    "    for parse_tree in parse_trees:\n",
    "        try:\n",
    "            graph = parse_tree_to_graph(parse_tree=parse_tree, display=False)\n",
    "        except Exception as e:\n",
    "            print(f'error: {parse_tree}')\n",
    "            with open(error_to, 'a', encoding='utf8') as f:\n",
    "                f.write(f'{parse_tree}\\t{e}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T15:56:41.966031Z",
     "start_time": "2022-04-28T15:55:45.233273Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(QUESTION_PARSE_TREE_PATH, sep='\\t', header=0)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    option_a_tree = eval(df.loc[i, 'option_a'])\n",
    "    option_b_tree = eval(df.loc[i, 'option_b'])\n",
    "    option_c_tree = eval(df.loc[i, 'option_c'])\n",
    "    option_d_tree = eval(df.loc[i, 'option_d'])\n",
    "    \n",
    "    parse_trees_to_graph(parse_trees=option_a_tree, succ_to='test_parse_tree_to_graph.txt', error_to='error_parse_tree_to_graph.txt')\n",
    "    parse_trees_to_graph(parse_trees=option_b_tree, succ_to='test_parse_tree_to_graph.txt', error_to='error_parse_tree_to_graph.txt')\n",
    "    parse_trees_to_graph(parse_trees=option_c_tree, succ_to='test_parse_tree_to_graph.txt', error_to='error_parse_tree_to_graph.txt')\n",
    "    parse_trees_to_graph(parse_trees=option_d_tree, succ_to='test_parse_tree_to_graph.txt', error_to='error_parse_tree_to_graph.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T15:57:37.731474Z",
     "start_time": "2022-04-28T15:57:16.339536Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(REFERENCE_PARSE_TREE_PATH, sep='\\t', header=0)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    trees = eval(df.loc[i, 'content'])\n",
    "    parse_trees_to_graph(parse_trees=trees, succ_to='test_parse_tree_to_graph.txt', error_to='error_parse_tree_to_graph.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句法树分支数量情况（最大子女数量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T14:52:30.910710Z",
     "start_time": "2022-09-05T14:52:00.025331Z"
    }
   },
   "outputs": [],
   "source": [
    "# 确定最大子女数(参考文档)\n",
    "\n",
    "df = pandas.read_csv(REFERENCE_PARSE_TREE_PATH, sep='\\t', header=0)\n",
    "logs = []\n",
    "for i in range(df.shape[0]):\n",
    "    content = eval(df.loc[i, 'content'])\n",
    "    for tree_string in content:\n",
    "        tree = parse_tree_to_graph(parse_tree=tree_string, display=False, return_type='networkx', ignore_text=False)\n",
    "        edges = list(tree.edges)\n",
    "        out_degrees = list(map(lambda x: x[0], edges))\n",
    "        temp_max = Counter(out_degrees).most_common(1)[0][1]\n",
    "        logs.append(temp_max)\n",
    "print(max(logs))\n",
    "print(sum(logs) / len(logs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文档最大子女数为47，均值为3.245742220908256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T06:36:48.624187Z",
     "start_time": "2022-09-06T06:36:00.908408Z"
    }
   },
   "outputs": [],
   "source": [
    "# 确定最大子女数(题库)\n",
    "\n",
    "df = pandas.read_csv(QUESTION_PARSE_TREE_PATH, sep='\\t', header=0)\n",
    "max_children = 0\n",
    "\n",
    "statement_logs = []\n",
    "option_logs = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    statement = eval(df.loc[i, 'statement'])\n",
    "    option_a = eval(df.loc[i, 'option_a'])\n",
    "    option_b = eval(df.loc[i, 'option_b'])\n",
    "    option_c = eval(df.loc[i, 'option_c'])\n",
    "    option_d = eval(df.loc[i, 'option_d'])\n",
    "    options = option_a + option_b + option_c + option_d\n",
    "    for tree_string in statement:\n",
    "        tree = parse_tree_to_graph(parse_tree=tree_string, display=False, return_type='networkx', ignore_text=False)\n",
    "        edges = list(tree.edges)\n",
    "        out_degrees = list(map(lambda x: x[0], edges))\n",
    "        temp_max = Counter(out_degrees).most_common(1)[0][1]\n",
    "        statement_logs.append(temp_max)\n",
    "    for tree_string in options:\n",
    "        tree = parse_tree_to_graph(parse_tree=tree_string, display=False, return_type='networkx', ignore_text=False)\n",
    "        edges = list(tree.edges)\n",
    "        out_degrees = list(map(lambda x: x[0], edges))\n",
    "        temp_max = Counter(out_degrees).most_common(1)[0][1]\n",
    "        option_logs.append(temp_max)\n",
    "\n",
    "print('statement:')\n",
    "print(max(statement_logs))\n",
    "print(sum(statement_logs) / len(statement_logs))\n",
    "print('options:')\n",
    "print(max(option_logs))\n",
    "print(sum(option_logs) / len(option_logs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题库中：\n",
    "- 题干最大子女数为26，均值为3.0357229795520935\n",
    "- 选项最大子女数为23，均值为2.8461790633467707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-05T14:44:14.786607Z",
     "start_time": "2022-09-05T14:44:14.773614Z"
    }
   },
   "outputs": [],
   "source": [
    "tree.edges.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句法树在Dataset中的生成情况测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 由于有的字符jieba可以识别而stanford句法解析不能识别，导致pos_tags的长度与分词编码长度不同，目前发现的特殊字符有：\\xa0（编号为692）, \\u3000（编号为26182）（在token2id.csv中是这个，但是这里的编码用的其实是reference_token2id.csv里的编码，那个token里面是没有\\xa0和\\u3000的），有点痛苦。\n",
    "2. 20220518更新: \\xa0可以被识别，但是单空格不能被stanford句法树识别，因此目前special_chars.txt中只有\\u3000与单空格。\n",
    "3. 20220518更新: 判定pos_tags有用字符数量的方式从`x!=''`改为`x in STANFORD_POS_TAG`, 更贴近原文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use_reference为False的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T05:03:59.368018Z",
     "start_time": "2022-05-20T05:03:55.491655Z"
    }
   },
   "outputs": [],
   "source": [
    "args = load_args(Config=DatasetConfig)\n",
    "args.use_reference = False\n",
    "args.retrieval_model_name = 'tfidf'\n",
    "args.train_batch_size = 2\n",
    "args.valid_batch_size = 2\n",
    "args.test_batch_size = 2\n",
    "args.use_parse_tree = True\n",
    "args.use_pos_tags = True\n",
    "args.word_embedding = None\n",
    "args.document_embedding = None\n",
    "pipeline = 'choice'\n",
    "mode = 'test'\n",
    "\n",
    "dataset = BasicDataset(args=args, mode=mode, do_export=False, pipeline=pipeline, for_debug=False)\n",
    "df = dataset.data\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T05:04:03.679209Z",
     "start_time": "2022-05-20T05:04:02.092764Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    options = df.loc[i, 'options']\n",
    "    statement = df.loc[i, 'question']\n",
    "    length_statement = len(list(filter(lambda x: x!=0 and x!='', statement)))\n",
    "    length_a = len(list(filter(lambda x: x not in [0], options[0])))\n",
    "    length_b = len(list(filter(lambda x: x not in [0], options[1])))\n",
    "    length_c = len(list(filter(lambda x: x not in [0], options[2])))\n",
    "    length_d = len(list(filter(lambda x: x not in [0], options[3])))\n",
    "    \n",
    "    statement_pos_tags = df.loc[i, 'statement_pos_tags']\n",
    "    option_a_pos_tags = df.loc[i, 'option_a_pos_tags']\n",
    "    option_b_pos_tags = df.loc[i, 'option_b_pos_tags']\n",
    "    option_c_pos_tags = df.loc[i, 'option_c_pos_tags']\n",
    "    option_d_pos_tags = df.loc[i, 'option_d_pos_tags']\n",
    "    \n",
    "#     length_statement_pos_tags = len(list(filter(lambda x: x!='', statement_pos_tags)))\n",
    "#     length_a_pos_tags = len(list(filter(lambda x: x!='', option_a_pos_tags)))\n",
    "#     length_b_pos_tags = len(list(filter(lambda x: x!='', option_b_pos_tags)))\n",
    "#     length_c_pos_tags = len(list(filter(lambda x: x!='', option_c_pos_tags)))\n",
    "#     length_d_pos_tags = len(list(filter(lambda x: x!='', option_d_pos_tags)))\n",
    "\n",
    "    \n",
    "    length_statement_pos_tags = len(list(filter(lambda x: x in STANFORD_POS_TAG, statement_pos_tags)))\n",
    "    length_a_pos_tags = len(list(filter(lambda x: x in STANFORD_POS_TAG, option_a_pos_tags)))\n",
    "    length_b_pos_tags = len(list(filter(lambda x: x in STANFORD_POS_TAG, option_b_pos_tags)))\n",
    "    length_c_pos_tags = len(list(filter(lambda x: x in STANFORD_POS_TAG, option_c_pos_tags)))\n",
    "    length_d_pos_tags = len(list(filter(lambda x: x in STANFORD_POS_TAG, option_d_pos_tags)))\n",
    "    \n",
    "    if not length_statement == length_statement_pos_tags:\n",
    "        print(df.loc[i, 'id'], 'STATEMENT')\n",
    "        print(statement, length_statement)\n",
    "        print(statement_pos_tags, length_statement_pos_tags)\n",
    "    \n",
    "    if not length_a == length_a_pos_tags:\n",
    "        print(df.loc[i, 'id'], 'A')\n",
    "        print(options[0], length_a)\n",
    "        print(option_a_pos_tags, length_a_pos_tags)\n",
    "        \n",
    "    if not length_b == length_b_pos_tags:\n",
    "        print(df.loc[i, 'id'], 'B')\n",
    "        print(options[1], length_b)\n",
    "        print(option_b_pos_tags, length_b_pos_tags)\n",
    "        \n",
    "    if not length_c == length_c_pos_tags:\n",
    "        print(df.loc[i, 'id'], 'C')\n",
    "        print(options[2], length_c)\n",
    "        print(option_c_pos_tags, length_c_pos_tags)\n",
    "\n",
    "    if not length_d == length_d_pos_tags:\n",
    "        print(df.loc[i, 'id'], 'D')\n",
    "        print(options[3], length_d)\n",
    "        print(option_d_pos_tags, length_d_pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的测试无输出表明就正确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use_reference为True的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T10:41:33.712893Z",
     "start_time": "2022-05-20T10:27:50.991577Z"
    }
   },
   "outputs": [],
   "source": [
    "args = load_args(Config=DatasetConfig)\n",
    "args.use_reference = True\n",
    "args.retrieval_model_name = 'tfidf'\n",
    "args.train_batch_size = 2\n",
    "args.valid_batch_size = 2\n",
    "args.test_batch_size = 2\n",
    "args.use_parse_tree = True\n",
    "args.use_pos_tags = True\n",
    "args.word_embedding = None\n",
    "args.document_embedding = None\n",
    "pipeline = 'choice'\n",
    "mode = 'train'\n",
    "\n",
    "dataset = BasicDataset(args=args, mode=mode, do_export=False, pipeline=pipeline, for_debug=False)\n",
    "df = dataset.data\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T10:50:49.130300Z",
     "start_time": "2022-05-20T10:49:04.316666Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    reference = df.loc[i, 'reference']\n",
    "    reference_pos_tags = df.loc[i, 'reference_pos_tags']\n",
    "    length_reference = [len(list(filter(lambda x: x not in [0], _reference))) for _reference in reference]\n",
    "    length_reference_pos_tags = [len(list(filter(lambda x: x in STANFORD_POS_TAG, _reference_pos_tags))) for _reference_pos_tags in reference_pos_tags]\n",
    "\n",
    "    if not length_reference == length_reference_pos_tags:\n",
    "        print(df.loc[i, 'id'], 'REFERENCE')\n",
    "        print(length_reference, length_reference_pos_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的测试无输出表明就正确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依存关系图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-16T09:14:14.296737Z",
     "start_time": "2022-07-16T09:14:13.919323Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_dependency_df = pandas.read_csv(REFERENCE_DEPENDENCY_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-16T09:14:14.343633Z",
     "start_time": "2022-07-16T09:14:14.328002Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_dependency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-16T09:16:16.131341Z",
     "start_time": "2022-07-16T09:16:16.115715Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "errors = []\n",
    "\n",
    "for dependency in reference_dependency_df['content']:\n",
    "    if not str(dependency)[0] == '[':\n",
    "        errors.append(str(dependency))\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "print(Counter(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "错误分析：\n",
    "- 'NoneType' object is not subscriptable: .triples()方法出错，这个问题很棘手，目前理解为解析错误，但是不知道具体怎么解决。\n",
    "- Number of tab-delimited fields (9) not supported by CoNLL(10) or Malt-Tab(4) format: 由于\\xa0特殊字符，需要删除\n",
    "- nan: 发生了断言错误，据观察也是与\\xa0，这个出现问题的都是句中连续的\\xa0，前一个则是句首。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-16T09:34:23.835088Z",
     "start_time": "2022-07-16T09:34:23.819143Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_dependency_df[reference_dependency_df['content'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT输出本地化的一些测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经确认的事实：\n",
    "\n",
    "- reference_bert_output共计24718行，全部都成功生成了BERT张量\n",
    "- question_bert_output共计26361行，全部都成功生成了BERT张量\n",
    "- 题库中训练集和测试集上的题目id没有重复的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T03:24:55.940205Z",
     "start_time": "2022-03-23T03:24:48.972819Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_bert_outputs = dill.load(open(REFERENCE_BERT_OUTPUT_PATH, 'rb'))\n",
    "question_bert_outputs = dill.load(open(QUESTION_BERT_OUTPUT_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T03:38:59.909228Z",
     "start_time": "2022-03-23T03:38:59.893375Z"
    }
   },
   "outputs": [],
   "source": [
    "len(set(question_bert_outputs['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T04:21:26.151363Z",
     "start_time": "2022-03-23T04:21:26.135598Z"
    }
   },
   "outputs": [],
   "source": [
    "len(question_bert_outputs['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T15:52:59.744697Z",
     "start_time": "2022-05-10T15:52:59.399731Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_parse_tree_df = pandas.read_csv(REFERENCE_PARSE_TREE_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
